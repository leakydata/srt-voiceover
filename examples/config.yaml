# Example configuration for srt-voiceover
# Copy this file and customize for your needs

# ===================================
# Voice Settings
# ===================================
default_voice: "en-US-AndrewMultilingualNeural"

# Speech adjustments (Edge TTS format)
rate: "+0%"      # Speed: -50% to +100% (e.g., "+20%" = 20% faster)
volume: "+0%"    # Volume: -50% to +100%
pitch: "+0Hz"    # Pitch: -50Hz to +100Hz

# Timing tolerance in milliseconds for audio alignment
timing_tolerance_ms: 150

# ===================================
# Whisper Transcription - OPTIONAL
# ===================================
# Uses LOCAL whisper by default (pip install openai-whisper)
whisper_model: "base"  # Model size: tiny, base, small, medium, large
use_whisper_api: false  # Set to true only if using OpenAI API

# Only needed if use_whisper_api is true:
# whisper_api_url: "https://api.openai.com/v1/audio/transcriptions"
# whisper_api_key: "sk-your-openai-api-key"

# ===================================
# Pyannote Speaker Diarization - OPTIONAL
# ===================================
# For professional-grade speaker detection
# Requires: 
#   1. pip install pyannote.audio
#   2. Set HF_TOKEN environment variable with your HuggingFace token
#      Get token at: https://huggingface.co/settings/tokens
#      Accept license: https://huggingface.co/pyannote/speaker-diarization-3.1

use_pyannote: false  # Set true to enable professional diarization

# ===================================
# Speaker to Voice Mapping
# ===================================
# Map speaker names to specific voices
# Works for both manual SRT labels and auto-detected speakers
speaker_voices:
  Nathan: "en-US-AndrewMultilingualNeural"
  Nicole: "en-US-EmmaMultilingualNeural"
  John: "en-US-GuyNeural"
  Sarah: "en-US-JennyNeural"
  Speaker A: "en-US-AndrewMultilingualNeural"   # Auto-detected
  Speaker B: "en-US-EmmaMultilingualNeural"     # Auto-detected
  
# See edgetts_voices_list.md for complete list of available voices
